\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
% \usepackage{setspace}
% \onehalfspacing
% \allowdisplaybreaks
%

\begin{document}
\begin{titlepage}
  \centering
  \hphantom{}\par
  \vspace{2cm}
  \includegraphics[width=0.15\textwidth]{logo.png}\par\vspace{0.5cm}
  {\LARGE \textsc{University of Oxford}\par}\vspace{0.5cm}
  {\Large \textsc{Group Design Practical}\par}\vspace{0.5cm}
  {\large \textsc{Group 14}\par}\vspace{0.7cm}
  {\Huge Machine Learning in the Browser with the BBC Micro:Bit\par}\vspace{0.8cm}
  {\large Louis-Emile Ploix\footnote{\href{mailto:louis-emile.ploix@stcatz.ox.ac.uk}{louis-emile.ploix@stcatz.ox.ac.uk}}, Ike Glassbrook\footnote{\href{mailto:isaac.glassbrook@lmh.ox.ac.uk}{isaac.glassbrook@lmh.ox.ac.uk}}, Joseph Simkin\footnote{\href{mailto:joseph.simkin@some.ox.ac.uk}{joseph.simkin@some.ox.ac.uk}}, Alikhan Murat\footnote{\href{mailto:alikhan.murat@magd.ox.ac.uk}{alikhan.murat@magd.ox.ac.uk}}, Andy van Horssen\footnote{\href{mailto:andy.vanhorssen@sjc.ox.ac.uk}{andy.vanhorssen@sjc.ox.ac.uk}}, and Ewan Hawkrigg\footnote{\href{mailto:ewan.hawkrigg@keble.ox.ac.uk}{ewan.hawkrigg@keble.ox.ac.uk}}\par}\vspace{0.7cm}
  {\large Internal supervisor: Qian Xie\par \vspace{0.3cm} External supervisor: Robert Knight \par}\vspace{1cm}
  {\Large May 2024}
\end{titlepage}

\tableofcontents

\section{Introduction}%
\label{sec:intro}

The Group Design Practical is a course taken by all 2nd year undergraduate students at the University of Oxford studying for a degree in Computer Science, Mathematics and Computer Science, or Computer Science and Philosophy. This report details the work of Group 14 from February to May 2024 to design, implement, and deploy a product satisfying the specification as provided by Micro:Bit.

\subsection{Technical context}%
\label{subsec:context}

Our project is based on a machine learning tool developed by researchers at the Centre for Computational Thinking and Design at Aarhus University. This tool is publicly deployed at \url{https://ml-machine.org}, with its source code available at \url{https://github.com/microbit-foundation/cctd-ml-machine}. Our project is a fork of this repository, extending the existing project. \\

ML-Machine is an application for users with access to a physical micro:bit device to train a machine learning model that uses the micro:bit's sensors as input data. The user is given a 3 step process:
\begin{enumerate}
        \item Named `Gestures' are added, for the trained model to eventually use as classification categories. The user then adds to each gesture several recordings of the accelerometer that correspond to the particular gesture. Alternatively, they can use the example dataset, containing the categories `shake' (with recordings of the micro:bit being shook), `still' (with recordings of the micro:bit not moving), and `circle' (with recordings of the micro:bit being moved in a circle).
  \item The user trains a model -- either a dense layers model, or a k-nearest neighbours model -- to classify the data. The recordings are pre-processed using a set of filters, which build the feature set. Before training, the user may select in particular which filters they would like to use.
        \item The micro:bit then has its sensors periodically polled in order to predict, of the labelled gestures, the one most similar to its current movement.
\end{enumerate}

The web application is written using the Svelte framework~\cite{svelte}, acting essentially as an HTML template language. The micro:bit itself requires additional drivers to interface with the application, as first a bluetooth connection must be established, and then the appropriate sensor data must be streamed to the application. These are written in C++, and then converted into \verb|.hex| files, which users can install on their micro:bits in order to allow this procedure to work. Both models are trained in the browser using TensorFlow.js~\cite{tensorflowjs}. \\

% Detail the current state, explaining the work of Aarhus University
% Explain the initial build of the project

\subsection{Project Specification}%
\label{subsec:spec}

In consideration of the existing context, the specifications set out by Micro:Bit were designed with a view to utilise our work for experimental purposes, and to provide a benchmark for evaluating the feasibility of future projects. On this basis, the specification gave the following requirements of a final product:
\begin{itemize}
  \item That a user should be able to train any model on not just the Micro:bit's accelerometer, but also an additional sensor.
        \begin{itemize}
                \item The user should be able to choose which sensor's data is to be streamed into the training data (and thus, which sensor's data is polled once the model is trained).
                \item Information given by the sensor should be visualised to the user in real-time in a manner which is understandable.
                \item The sensor data should be amenable to machine-learning analysis via the models available.
        \end{itemize}
  \item In addition to the Dense Neural Network, and the k-Nearest Neighbours models of the base application, a new neural network architecture should be investigated for implementation.
        \begin{itemize}
                \item This network should be capable of predicting on simple patterns with reasonable accuracy.
                \item The technical details of the network should be of pedagogical value, in addition to being amenable to high-level explanation.
                \item The trained model should be of a size that could fit on the micro:bit itself, rather than needing to run on a connected device\footnote{The Micro:Bit team have expressed a desire to run the prediction models directly on the micro:bit in future, rather than in the browser, allowing for the model to be stored independently of the application. This was not itself within the scope of our specification however.}.
                \item Model training should be responsive on standard browsers ran on computers with low-end modern hardware.
        \end{itemize}
\end{itemize}

\section{Logistics}%
\label{sec:logistics}

This section describes the timeline of our development, and how we went about structuring the delegation of work.

\subsection{Timeline}%
\label{subsec:timeline}

%\begin{itemize}
%        \item February 2nd -- First meeting with both supervisors. The project and existing code were explained, with directions given to relevant tools. D
%\end{itemize}

\subsection{Role Delegation}%
\label{subsec:delegation}

\section{Implementation}%
\label{sec:implementation}

\subsection{Streaming of new sensors from the micro:bit}%
\label{subsec:streaming}
We added support for two new sensors to the application: The magnetometer and the light sensor. In each case the sensor data needed to be sampled on the micro:bit and sent over bluetooth to the browser which is using the Web Bluetooth API ~\cite{bluetoothapi}. The magnetometer was straightforward in that it has a dedicated Bluetooth Service ~\cite{microbitservices}, an API for which is provided by the Lancaster Universitiy micro:bit Runtime ~\cite{magnetometerservice}. Light sensing has no associated bluetooth service, so we sent the sampled data manually instead: To do this we start a 'fiber' (a lightweight thread) when the micro:bit boots. This fiber repeatedly samples the environment brightness, sends this to the UART and sleeps for a few milliseconds.

For both sensors, the incoming data in the browser will be parsed, normalised and finally sent to an associated LiveData buffer. The LiveData buffers act as the sources of data for several other components of the application.

\subsection{Normalisation of data}%
\label{subsec:datanorm}
The data receieved by the browser will be the raw data as read by the microbit. This means it needs to be processed before it can be fed into other regions of the application which expect values to be of a specific range. In particular the magnetometer presented difficulties in it's sensitivity and discontinuity at times.

Our initial attempt was a straightforward division into the required range. This was extremely sensitive, but also jumped discontinuously at times. We subsequently tried a logarithmic scaling, but this suffered from largely the same problems. We then realised that dropping the sign of each component of the vector dealt with the discontinuous jumps. This suggested that we were treating the data as signed when it should have been unsigned, but this does not match with any of the internals or documentation of the micro:bit runtime. This too remained incredibly sensitive to even small movements. Finally, we settled on taking the \verb|atan2| of the absolute of each pair of components. This is nice in that it has a physical interpretation: the x component would be the compass heading, and the \verb|y|, \verb|z| components would be equivalents on other axis.

\subsection{Visualisation of new sensors}%
\label{subsec:sensorvis}

\subsection{Sensor choice in the UI}%
\label{subsec:sensorchoice}
After discussion with the micro:bit team it was decided that we wanted to allow the user of the application to be able to select which combination of sensors they wanted to be able to train the model with. It was important that this was done without compromising UX, in particular we did not want to force the user to make a decision they did not necessarily understand at the time. This was accomplished by adding a new page to the start of the pipeline the user follows, in which the user was given a clear choice of options with sensible defaults and help options if they wanted them. If no sensors were selected the user would be unable to train or record any gestures, and would instead be invited to do so. We also added indicators to each stage of the pipeline to help the user understand which stages were using which sensors. This is a something we found to be a problem when testing the application ourselves. Finally we decided that if the sensor selection was changed we would clear the currently trained model to ensure that confusion over what was being used was completely avoided.

% includes filter adaptation

\subsection{Visualisation of new sensors}%
\label{subsec:sensorvis}
The web application displays a graph of the data being recieved in the browser from the micro:bit. This initially just contained the \verb|x|, \verb|y|, \verb|z| components of the accelerometer. We found that after adding the magnetometer and light sensor to the graph it became quite difficult to read, with now 7 lines displayed on the same graph. To deal with this we split the graph into three smaller graphs, one for each sensor, stacked on top of one another. This is much clearer to the user. We also now gray out the graphs not currently selected. Note that this does not mean that the live data isn't being rendered anyway. This distinction is important to help indicate to the user that the application is recording data for gestures from each sensor regardless of whether it is currently being used. This means that the user can change the sensor selection without re-recording data.

\subsection{Application deployment}%
\label{subsec:deployment}

\section{Areas of Further Development}%
\label{sec:development}

\section{Concluding Remarks}%
\label{sec:conclusion}

\bibliographystyle{IEEETran}
\raggedright\bibliography{main}

\end{document}
